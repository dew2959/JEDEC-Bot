"""
Test Agentic Search Workflow for JEDEC Insight
Tests the complete agentic search system with JESD21C knowledge integration.
"""

import asyncio
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

from src.models.enhanced_rag_engine import create_enhanced_rag_engine
from src.utils.agentic_search import perform_agentic_search
from src.utils.category_detector import analyze_user_query


async def test_agentic_workflow():
    """Test the complete agentic search workflow."""
    print("ğŸ¤– JEDEC Insight Agentic Workflow Test")
    print("=" * 70)
    
    try:
        # Initialize enhanced RAG engine
        engine = await create_enhanced_rag_engine()
        
        # Test agentic search scenarios
        test_scenarios = [
            {
                "name": "JESD21C ê¸°ë³¸ ìš©ì–´ ê²€ìƒ‰",
                "query": "ESD Class 1 ìš”êµ¬ì‚¬í•­",
                "expected_behavior": "JESD21C ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ìš©ì–´ ì •ì˜ í›„ ê²€ìƒ‰",
                "test_focus": "knowledge_base_lookup"
            },
            {
                "name": "ëª¨í˜¸í•œ ê¸°ìˆ  ìš©ì–´ í•´ê²°",
                "query": "HBM í…ŒìŠ¤íŠ¸ ë°©ë²•",
                "expected_behavior": "ìš©ì–´ ë¶„ì„ í›„ ì—¬ëŸ¬ ê²€ìƒ‰ ì „ëµ ì‹œë„",
                "test_focus": "terminology_resolution"
            },
            {
                "name": "JESD ê´€ë ¨ ë¹„êµ ì§ˆì˜",
                "query": "ESDì™€ CDMì˜ ì°¨ì´ì  ë¹„êµ",
                "expected_behavior": "ë¹„êµ ë¶„ì„ê³¼ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰",
                "test_focus": "comparison_analysis"
            },
            {
                "name": "ì¼ë°˜ JEDEC ê·œê²© ê²€ìƒ‰",
                "query": "JEDEC ë™ì‘ ì „ì•• ê·œê²©",
                "expected_behavior": "Common ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ ë° JESD21C ì§€ì‹ í™œìš©",
                "test_focus": "category_integration"
            },
            {
                "name": "ë³µí•© ê¸°ìˆ  ì§ˆì˜",
                "query": "IC íŒ¨í‚¤ì§•ê³¼ ESD ë³´í˜¸ ìš”êµ¬ì‚¬í•­",
                "expected_behavior": "Packageì™€ Common ì¹´í…Œê³ ë¦¬ í†µí•© ê²€ìƒ‰",
                "test_focus": "multi_category_search"
            },
            {
                "name": "ì‹¤ë¬´ ì‹œë‚˜ë¦¬ì˜¤",
                "query": "ì œí’ˆ ì¶œí•˜ ì‹œ ESD ë³´í˜¸ ì ˆì°¨",
                "expected_behavior": "ì‹¤ë¬´ ìƒí™©ì— ë§ëŠ” ê²€ìƒ‰ ì „ëµ",
                "test_focus": "practical_application"
            }
        ]
        
        print(f"ğŸ§ª ì´ {len(test_scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
        print()
        
        results = []
        
        for i, scenario in enumerate(test_scenarios, 1):
            print(f"ğŸ” ì‹œë‚˜ë¦¬ì˜¤ {i}: {scenario['name']}")
            print(f"ì§ˆì˜: {scenario['query']}")
            print(f"ì˜ˆìƒ ë™ì‘: {scenario['expected_behavior']}")
            print("-" * 50)
            
            try:
                # Perform agentic search
                start_time = asyncio.get_event_loop().time()
                
                result = await perform_agentic_search(
                    scenario['query'], 
                    engine, 
                    analyze_user_query,
                    k=5
                )
                
                end_time = asyncio.get_event_loop().time()
                search_time = end_time - start_time
                
                # Analyze results
                workflow = result.get("search_workflow", [])
                final_result = result.get("final_result", {})
                
                # Evaluate success based on expected behavior
                success_criteria = {
                    "terminology_found": len(final_result.get("terminology_enhanced", [])) > 0,
                    "knowledge_used": final_result.get("knowledge_used", False),
                    "multiple_steps": len(workflow) > 3,
                    "sources_found": len(final_result.get("sources", [])) > 0,
                    "confidence_adequate": final_result.get("confidence", 0) > 0.6
                }
                
                # Scenario-specific success criteria
                scenario_success = True
                issues = []
                
                if scenario["test_focus"] == "knowledge_base_lookup":
                    if not success_criteria["knowledge_used"]:
                        scenario_success = False
                        issues.append("JESD21C ì§€ì‹ë² ì´ìŠ¤ ë¯¸í™œìš©")
                
                elif scenario["test_focus"] == "terminology_resolution":
                    if not success_criteria["terminology_found"]:
                        scenario_success = False
                        issues.append("ê¸°ìˆ  ìš©ì–´ ë¶„ì„ ì‹¤íŒ¨")
                
                elif scenario["test_focus"] == "comparison_analysis":
                    if not final_result.get("is_comparison", False):
                        scenario_success = False
                        issues.append("ë¹„êµ ë¶„ì„ ë¯¸ìˆ˜í–‰")
                
                elif scenario["test_focus"] == "category_integration":
                    if "Common" not in [s.get("document_id", "") for s in final_result.get("sources", [])]:
                        scenario_success = False
                        issues.append("Common ì¹´í…Œê³ ë¦¬ ë¬¸ì„œ ë¯¸ê²€ìƒ‰")
                
                # Overall quality assessment
                quality_score = 0
                if success_criteria["terminology_found"]:
                    quality_score += 25
                if success_criteria["knowledge_used"]:
                    quality_score += 25
                if success_criteria["multiple_steps"]:
                    quality_score += 20
                if success_criteria["sources_found"]:
                    quality_score += 20
                if success_criteria["confidence_adequate"]:
                    quality_score += 10
                
                # Store test result
                test_result = {
                    "scenario_id": i,
                    "scenario_name": scenario["name"],
                    "query": scenario['query'],
                    "expected_behavior": scenario['expected_behavior'],
                    "search_time": search_time,
                    "workflow_steps": len(workflow),
                    "success": scenario_success,
                    "quality_score": quality_score,
                    "confidence": final_result.get("confidence", 0),
                    "sources_count": len(final_result.get("sources", [])),
                    "terminology_enhanced": final_result.get("terminology_enhanced", False),
                    "knowledge_used": final_result.get("knowledge_used", False),
                    "issues": issues,
                    "workflow": result.get("search_workflow", {}),
                    "final_result": final_result
                }
                
                results.append(test_result)
                
                # Display results
                status_icon = "âœ…" if scenario_success else "âŒ"
                print(f"{status_icon} ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼:")
                print(f"  ê²€ìƒ‰ ì‹œê°„: {search_time:.2f}ì´ˆ")
                print(f"  ì›Œí¬í”Œë¡œìš° ë‹¨ê³„: {len(workflow)}")
                print(f"  ì‹ ë¢°ë„: {final_result.get('confidence', 0):.2f}")
                print(f"  í’ˆì§ˆ ì ìˆ˜: {quality_score}/100")
                print(f"  ì¶œì²˜ ë¬¸ì„œ: {len(final_result.get('sources', []))}ê°œ")
                
                if final_result.get("terminology_enhanced"):
                    print(f"  ğŸ§  ìš©ì–´ í–¥ìƒ ì ìš©ë¨")
                
                if final_result.get("knowledge_used"):
                    print(f"  ğŸ“š JESD21C ì§€ì‹ í™œìš©ë¨")
                
                if issues:
                    print(f"  âš ï¸ ë¬¸ì œì :")
                    for issue in issues:
                        print(f"    - {issue}")
                
                # Show workflow steps summary
                if workflow:
                    print(f"  ğŸ”„ ì›Œí¬í”Œë¡œìš°:")
                    for step in workflow:
                        step_name = step.get("step", "Unknown")
                        step_confidence = step.get("confidence", 0)
                        step_desc = step.get("description", "")
                        print(f"    {step_name}: ì‹ ë¢°ë„ {step_confidence:.2f}")
                
                print()
                
            except Exception as e:
                print(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ ì˜¤ë¥˜: {e}")
                results.append({
                    "scenario_id": i,
                    "scenario_name": scenario['name'],
                    "query": scenario['query'],
                    "error": str(e),
                    "success": False
                })
            
            print("=" * 50)
            print()
        
        # Summary analysis
        successful_scenarios = sum(1 for r in results if r.get('success', False))
        total_scenarios = len(results)
        
        print("ğŸ“Š ì—ì „íŠ¸ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 70)
        print(f"ì´ ì‹œë‚˜ë¦¬ì˜¤: {total_scenarios}")
        print(f"ì„±ê³µ: {successful_scenarios}")
        print(f"ì‹¤íŒ¨: {total_scenarios - successful_scenarios}")
        print(f"ì„±ê³µë¥ : {(successful_scenarios/total_scenarios)*100:.1f}%")
        print()
        
        # Workflow step analysis
        step_analysis = {}
        for result in results:
            if "workflow" in result:
                workflow = result["workflow"]
                for step in workflow:
                    step_name = step.get("step", "Unknown")
                    if step_name not in step_analysis:
                        step_analysis[step_name] = {
                            "total": 0,
                            "success": 0,
                            "avg_confidence": 0
                        }
                    
                    step_analysis[step_name]["total"] += 1
                    if step.get("confidence", 0) > 0.5:
                        step_analysis[step_name]["success"] += 1
                    
                    step_analysis[step_name]["avg_confidence"] += step.get("confidence", 0)
        
        print("ğŸ“ˆ ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ë³„ ì„±ëŠ¥:")
        for step_name, stats in step_analysis.items():
            if stats["total"] > 0:
                success_rate = (stats["success"] / stats["total"]) * 100
                avg_conf = stats["avg_confidence"] / stats["total"]
                print(f"  {step_name}: ì„±ê³µë¥  {success_rate:.1f}%, í‰ê·  ì‹ ë¢°ë„ {avg_conf:.2f}")
        
        print()
        print("ğŸ‰ ì—ì „íŠ¸ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
        # Recommendations
        if successful_scenarios < total_scenarios:
            print("\nğŸ’¡ ê°œì„  ì œì–¸:")
            failed_scenarios = [r for r in results if not r.get('success', False)]
            
            if failed_scenarios:
                common_issues = {}
                for result in failed_scenarios:
                    for issue in result.get("issues", []):
                        common_issues[issue] = common_issues.get(issue, 0) + 1
                
                print("ê°€ì¥ ë§ì€ ë¬¸ì œ:")
                for issue, count in sorted(common_issues.items(), key=lambda x: x[1], reverse=True):
                    print(f"  - {issue}: {count}íšŒ")
                
                print("\nê°œì„  ë°©ì•ˆ:")
                print("  1. JESD21C ì§€ì‹ë² ì´ìŠ¤ í™•ì¥")
                print("  2. ìš©ì–´ ë¶„ì„ ì •í™•ë„ í–¥ìƒ")
                print("  3. ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ë³„ ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_agentic_workflow())
